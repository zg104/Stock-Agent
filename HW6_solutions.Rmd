---
title: "Homework 6"
author: "Zijing Gao"
output: pdf_document
---

# Problem 1

```{r}
library(dplyr, quietly = T)
library(plyr, quietly = T)
library(ggplot2, quietly = T)
```

## (a)

Let $\omega$ be the configuration our Metropolis-Hastings chain is currently in and let $\omega'$ be the proposed configuration.  Let the probability of proposing $\omega'$ be $p(\omega, \omega')$.  We generate our proposal by uniformly selecting a row and column, respectively, out of the $100$ rows and $100$ columns.  We can therefore propose $100^2$ different proposals, and each proposed configuration is equally likely.  Therefore $p(\omega, \omega') = 1/(100^2)$.  And similarly, $p(\omega', \omega) = 1/(100^2)$.

We want to draw from the stationary distribution $\nu_\omega = P(X = \omega)$ which is $1/|H|$ if $\omega \in H$ and $0$ otherwise.   

Putting these together, and assuming that $\omega$ satisfies the hard core restriction, we have the Metropolis-Hastings ratio,
\begin{equation}
\min(1, \frac{\nu_{\omega'} p(\omega', \omega)}{\nu_\omega p(\omega, \omega')})
= \min(1, \frac{\nu_{\omega'}}{\frac{1}{H}}) = \bigg\{
\begin{array}{cc}
0 & \text{if } \omega' \notin H \\ \notag
1 & \text{if } \omega' \in H
\end{array}
\end{equation}

In this case, the Metropolis-Hastings algorithm is simple, if our proposal satisfies the hard core configuration we accept it, otherwise we reject it.
```{r, cache=T}

HardCore_MCMC <- function(start_omega, num_time_steps, f=NULL,
                          num_rows=100, num_cols=100)
{
  omega <- start_omega
  # keep path values of f(omega) if f is not NULL
  if (!is.null(f)) {
    f_vals <- rep(NA, num_time_steps+1) %>% as.numeric
    f_vals[1] <- f(omega)
  } else
    f_vals <- NA
  
  for (i in 1:num_time_steps) {
    k <- sample.int(num_rows, 1)
    j <- sample.int(num_cols, 1)
    
    if (omega[k,j]==1) 
      omega[k,j] <- 0
    else {
      # see if flipping entry (i,j) satisfies hard core
      ns <- omega[k,max(1,j-1)] +
            omega[k,min(num_cols,j+1)] +
            omega[max(1,k-1),j] +
            omega[min(num_rows,k+1),j]
      if (ns == 0)
        omega[k,j] <- 1
    }
    
    if (!is.null(f))
      f_vals[i+1] <- f(omega)
  }
  
  return (list(sample=omega, f_vals=f_vals))
}

# let's see if plot of f(X(t)) over 100,000 time steps
start_omega <- matrix(0, nrow=100, ncol=100)
f_num_ones <- function(m) sum(as.numeric(m))/1E4
mcmc_out <- HardCore_MCMC(start_omega, num_time_steps = 1E5, 
                          f=f_num_ones)

plot(mcmc_out$f_vals, xlab="time step", ylab="number of 1's", cex=.5)
```

Let's zoom in a bit to see the $f(X(t)$ values once $t$ is greater than $50,000$.
```{r, cache=T}
plot(50000:1E5, mcmc_out$f_vals[50000:1E5], xlab="time step", ylab="number of 1's", cex=.5)
```

Looks pretty good, so I'll use $100,000$ as the sampling time.
```{r, cache=T}
image(mcmc_out$sample)
```

And now let's sample $250$ times and generate a histogram
```{r, cache=T}
N <- 250
f_samples <- rep(0, N)
for (i in 1:N) {
  omega <- HardCore_MCMC(start_omega, 1E5)$sample
  f_samples[i] <- f_num_ones(omega)
}
hist(f_samples)
```

## (b)

We'll use the same proposal, but now $\nu_\omega = \alpha f(\omega)^2$, making our Metropolis-Hastings ratio,
\begin{equation}
\min(1, \frac{\nu_{\omega'} p(\omega', \omega)}{\nu_\omega p(\omega, \omega')})
= \min(1, \frac{\nu_{\omega'}}{\nu_\omega}) = min(1, \frac{f(\omega')^2}{f(\omega)^2})
\end{equation}

```{r,cache=T}
Partb_MCMC <- function(start_omega, num_time_steps, f=NULL,
                          num_rows=100, num_cols=100)
{
  omega <- start_omega
  omega_num_1s <- colSums(omega) %>% sum
  # keep path values of f(omega) if f is not NULL
  if (!is.null(f)) {
    f_vals <- rep(NA, num_time_steps+1) %>% as.numeric
    f_vals[1] <- f(omega)
  } else
    f_vals <- NA

  for (i in 1:num_time_steps) {
    k <- sample.int(num_rows, 1)
    j <- sample.int(num_cols, 1)

    current_bit <- omega[k,j]
    proposal_num_1s <- omega_num_1s + current_bit

    MH_ratio <- (proposal_num_1s/omega_num_1s)^2
    if (runif(1) < MH_ratio) {
      omega[k,j] <- 1 - omega[k,j]
      omega_num_1s <- proposal_num_1s
    }

    if (!is.null(f))
      f_vals[i+1] <- f(omega)
  }

  return (list(sample=omega, f_vals=f_vals))
}

# Notice that here there is probability 0 of a configuration of all 0's,
# so we need to start in a different configuration
start_omega <- matrix(round(runif(100^2)), nrow=100, ncol=100)
mcmc_out <- Partb_MCMC(start_omega, num_time_steps = 1E5,
                          f=f_num_ones)
plot(50000:1E5, mcmc_out$f_vals[50000:1E5], xlab="time step", ylab="number of 1's", cex=.5)
```

Looks good, let's see a sample and produce the histogram.
```{r,cache=T}
image(mcmc_out$sample)
```

And now let's sample $250$ times and generate a histogram
```{r, cache=T}
N <- 250
f_samples <- rep(0, N)
for (i in 1:N) {
  omega <- Partb_MCMC(start_omega, 1E5)$sample
  f_samples[i] <- f_num_ones(omega)
}
hist(f_samples)
```

The fraction of configuration entries that are $1$'s is distributed roughly around $1/2$.  The probability distribution makes configurations with more $1$'s more likely, but most configurations have roughly an equal number of $1$'s and $0$'s, making the median roughly $1/2$.  This is in contrast to the hard-core model, where the fraction of entries tha are $1$'s is roughly $1/4$ due to the hard core restriction.  

# Problem 2

## (a)

```{r}
discrete_sample <- function(v=c(1/2, 1/3, 1/6))
{
  v_cs <- cumsum(v)
  sample <- which.max(runif(1) < v_cs)
  
  return (sample)
}


# let's check the sampler
N <- 1000
f_samples <- replicate(N, discrete_sample())
probs <- sapply(1:3, function(i) sum(f_samples==i)/N)
probs
true_probs <- c(1/2, 1/3, 1/6)
true_probs
```

Looks good!

## (b)

Let $R$ and $M$ be the transition probability matrices of the proposal and Metropolis-Hastings Markov chain $X(t)$, respectively.   If $X(t)=i$, then $X(t+1)=j$ if we propose $j$, which occurs with probability $R_{ij}$ and if we accept the proposal, which occurs with probability equal to the Metropolis-Hastings ratio.  Then for $i \ne j$, we have
\begin{equation}
M_{ij} = R_{ij} \min(1, \frac{\nu_j R_{ji}}{\nu_i R_{ij}})
\end{equation}

```{r,cache=T}
Problem2_MCMC <- function(start_state, num_steps)
{
  X <- start_state
  R_row <- c(.99, 009, .001)
  v <- c(1/2, 1/3, 1/6)
  
  for (i in 1:num_steps) {
    proposal <- sample(1:3, 1, prob=R_row)
    MH_ratio <- v[proposal]*R_row[X]/(v[X]*R_row[proposal])
    if (runif(1) < MH_ratio)
      X <- proposal
  }
  
  return (X)
}

# let's see if 100 time steps is enough
N <- 1000
f_samples <- sapply(1:N, function(i) Problem2_MCMC(1, num_steps=100))
probs <- sapply(1:3, function(i) sum(f_samples==i)/N)
probs
true_probs <- c(1/2, 1/3, 1/6)
true_probs
```

Not long enough!  Let's try $1000$ time steps
```{r,cache=T}
N <- 1000
f_samples <- sapply(1:N, function(i) Problem2_MCMC(1, num_steps=1000))
probs <- sapply(1:3, function(i) sum(f_samples==i)/N)
probs
true_probs <- c(1/2, 1/3, 1/6)
true_probs
```

Closer, but still not accurate enough.  Let's try $10,000$ time steps.
```{r,cache=T}
N <- 1000
f_samples <- sapply(1:N, function(i) Problem2_MCMC(1, num_steps=10000))
probs <- sapply(1:3, function(i) sum(f_samples==i)/N)
probs
true_probs <- c(1/2, 1/3, 1/6)
true_probs
```

This gets us fairly close, so we can say roughly $10,000$ time steps are needed.  Compare this enormous computational expenditure, to the discrete sample in (a), for which we only need to sample a single uniform r.v.

Finally, let's compute $M$, the transition probabiliy matrix of the Metropolis-Hastings chain
```{r}
MH_TPM <- function(R, v)
{
  n <- nrow(R)
  M <- matrix(0, nrow=n, ncol=n)
  
  for (i in 1:n)
    for (j in 1:n) 
      if (i != j) {
         MH_ratio <- min(1,v[j]*R[j,i]/(v[i]*R[i,j]))
         M[i,j] <- R[i,j]*MH_ratio
      }
  
  diag(M) <- 1 - rowSums(M)
  return (M)
}

R <- matrix(c(.99, 009, .001), nrow=3, ncol=3, byrow=T)
v <- c(1/2, 1/3, 1/6)
M <- MH_TPM(R, v)
print(M)
```

Compare $R$,
```{r}
R
```


Notice the inefficiency of $M$.  The chain rarely gets to state $3$ and once there rarely leaves.  This reflects the poor quality of the proposal.  The proposal rarely proposes state $3$.  In turn, to force the Metropolis-Hastings chain to reach a stationary distribution with $1/6$ probability of state $3$ requires that the Metropolis-Hastings chain stay in state $3$ for a long time.  