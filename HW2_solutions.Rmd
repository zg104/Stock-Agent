---
title: "Homework 2"
author: "Zijing Gao"
output: pdf_document
---

# Problem 1

## (a)

The cdf of an exponential is $F(x) = 1 - \exp[-\lambda x]$.  Let's solve for the inverse.
\begin{align}
c & = 1 - \exp[-\lambda F^{-1}(c)]. \\
\end{align}
Giving,
\begin{equation}
F^{-1}(c) = -\frac{1}{\lambda}\log(1-c)
\end{equation}

```{r, cache=T}
t <- proc.time()
x <- rexp(10^6, rate=1)
print(proc.time() - t)

t <- proc.time()
x <- -log(1 - runif(10^6))
print(proc.time() - t)
```
Both have similar user run times.

## (b)

First compute the cdf.
\begin{equation}
F(x) = \bigg\{
\begin{array}{cc}
\int_{-\infty}^x \frac{1}{2} \exp[z] dz & \text{ for } x \le 0 \\
\int_{-\infty}^0 \frac{1}{2} \exp[z] dz + \int_{0}^x \frac{1}{2} \exp[-z] dz 
            & \text{ for } x > 0 \\
\end{array}
\end{equation}

Computing the integrals gives
\begin{equation} \label{E:2}
F(x) = \bigg\{
\begin{array}{cc}
\frac{1}{2} \exp[x]  & \text{ for } x \le 0 \\
 1 - \frac{1}{2} \exp[-x] 
            & \text{ for } x > 0 \\
\end{array}
\end{equation}

Now invert each case separately.  When $x < 0$, $F(x) \le \frac{1}{2}$.  When $x > 0$, $F(x) \ge \frac{1}{2}$.  So plugging in $x = F^{-1}(c)$ we get the cases for $c$. 
\begin{equation}
F^{-1}(c) = \bigg\{
\begin{array}{cc}
\log(2c)  & \text{ for } c \le \frac{1}{2} \\
 -\log(2(1-c)) 
            & \text{ for } c \ge \frac{1}{2} \\
\end{array}
\end{equation}

```{r}
inversion_samples <- function(n)
{
  U <- runif(n)
  samples <- ifelse(U < 1/2, log(2*U), -log(2*(1-U)))
  
  return (samples)
}

inversion_samples(5)
```

Let's show that this sampling strategy is correct.  Let $Y$ be the r.v. produced by the sampling algorithm, then we need to show that the cdf of $Y$ is the same as the cdf of $X$.  First
\begin{equation}
P(Y \le x) = \bigg\{
\begin{array}{c}
P(\log(2U) \le x, U \le \frac{1}{2}) \\
P(-\log(2(1-U)) \le x,  U > \frac{1}{2}).
\end{array}
\end{equation}

Now break into cases depending on the sign of $x$.  If $x < 0$, then only $\log(2U)$ can be less than $x$, since $-\log(2(1-U)) > 0$ when $U \ge \frac{1}{2}$.   If $x > 0$, then $\log(2U)$ is always less then $x$ when $U \le \frac{1}{2}$.
\begin{equation} \label{E:1}
P(Y \le x) = \bigg\{
\begin{array}{cc}
P(\log(2U) \le x, U \le \frac{1}{2}) & \text{ if } x \le 0\\
\frac{1}{2} + P(-\log(2(1-U)) \le x,  U > \frac{1}{2}) & \text{otherwise}.
\end{array}
\end{equation}

Now the rest is just computations on the probability of $U$. If $x \le 0$,
\begin{equation}
P(\log(2U) \le x, U \le \frac{1}{2}) = P(U \le \frac{1}{2}\exp[x]) = \frac{1}{2} \exp[x].
\end{equation}
If $x > 0$,
\begin{equation}
\frac{1}{2} + P(-\log(2(1-U)) \le x, U > \frac{1}{2}) = \frac{1}{2} + P(\frac{1}{2} \le U \le 1 - \frac{1}{2}\exp[-x]) = 1 -  \frac{1}{2} \exp[-x].
\end{equation}

Plugging this into (\ref{E:1}) and comparing to (\ref{E:2}) shows that $F(Y \le x) = F(X \le x)$.


# Problem 2

Here's a solution using R's sampler.

```{r}
MarkovChain <- function(P, s0, s)
{
  x <- s0
  path <- x
  n_states <- nrow(P)
  
  while(x != s) {
    x <- sample.int(1, n=n_states, prob=P[x,])
    # concatenating is slow, but simple
    path <- c(path, x)
  }
  
  return (path)
}

# testing
P <- matrix(c(.5, .4, .1, 1, 0, 0, 0, .5, .5), byrow=T, nrow=3)
P
MarkovChain(P, 1, 3)
```


Here's a solution with a sampler.
```{r}
library(magrittr)

sample_discrete <- function(states, probs)
{
  cum_probs <- cumsum(probs)
  U <- runif(1)
  sampled_state <- states[(cum_probs > U) %>% which %>% min]
  
  return (sampled_state)
}

MarkovChain_with_sampler <- function(P, s0, s)
{
  x <- s0
  path <- x
  states <- 1:nrow(P)
  
  while(x != s) {
    x <- sample_discrete(states, probs=P[x,])
    # concatenating is slow, but simple
    path <- c(path, x)
  }
  
  return (path)
}
```

Let's check sample_discrete with a histogram.
```{r, cache=T}
states <- 1:3
probs <- c(.5, .2, .3)
samples <- replicate(1000, sample_discrete(states, probs))
hist(samples, probability=T, ylim=c(0,1), breaks=0:3)
```

Looks good!

```{r}
MarkovChain_with_sampler(P, 1, 3)
```

# Problem 3

```{r}
make_CL_transition_probability_matrix <- function()
{
  # start with a matrix with 106 squares 
  roll_P <- matrix(0, nrow=106, ncol=106)
  for (i in 1:100)
    roll_P[i,(i+1):(i+6)] <- 1/6
  
  # now make squares 101 to 106 land on 100
  roll_P[,100] <- roll_P[,100] + rowSums(roll_P[,101:106])
  roll_P <- roll_P[1:100,1:100]
  
  chutes_and_ladders <- read.csv("chutes_and_ladder_locations.csv", header=T)
  
  # make transition probability matrix for just chutes and ladders
  cl_P <- diag(100)
  for (i in 1:nrow(chutes_and_ladders)) {
    start <- chutes_and_ladders$start[i]
    end <- chutes_and_ladders$end[i]
    cl_P[start, start] <- 0
    cl_P[start, end] <- 1
  }
  
  # make final matrix by multiping, 
  # transitions have two steps:  roll then move on chutes/ladders
  P <-  roll_P %*% cl_P

  return (P)
}

# Get lenght of path of chutes and ladders game
# P : chutes and ladders transition probability matrix
simulate_num_rolls <- function(P)
{
  first_square <- sample.int(1, n=6)
  path <- MarkovChain(P, first_square, 100)
  
  num_rolls <- length(path) + 1
  return (num_rolls)
}
```

```{r, cache=T}
# simulate game length 10 times, just to see
P_CL <- make_CL_transition_probability_matrix()
samples <- replicate(10, simulate_num_rolls(P_CL))
print(samples)
```

Now let's find the expected value!
```{r, cache=T}
N <- 10000
samples <- replicate(N, simulate_num_rolls(P_CL))
# average number of rolls
print(mean(samples))
```

Let's compute an estimate of the variance to use for determining the CI.
```{r}
sigma2 <- var(samples)
print(sigma2)
```

To determine the number of samples we need, first recall the CLT argument for construction of the CI.  Let $X$ be the length of a chutes and ladders game.   Then we need to find $a, b$ such that
\begin{align}
P(a \le \left(\frac{1}{N} \sum_{i=1}^N \hat{X}_i - E[X] \right) \le b) = 0.95.
\end{align}

We apply the CTL scaling by multiplying by $\sqrt{N}$,
\begin{align}
P(a \sqrt{N} \le \sqrt{N}\left(\frac{1}{N} \sum_{i=1}^N \hat{X}_i - E[X] \right) \le  b \sqrt{N}) & = 0.95.
\end{align}
The term $\sqrt{N}\left(\frac{1}{N} \sum_{i=1}^N \hat{X}_i - E[X] \right)$ is exactly the form of the CTL, so we can approximate with a normal.
\begin{align}
P(a \sqrt{N} \le \mathcal{N}(0,\sigma^2) \le  b \sqrt{N}) & = 0.95.\\
\end{align}
Then pull out the $\sigma^2$,
\begin{align}
P(a \sqrt{N} \le \sigma \mathcal{N}(0,1) \le  b \sqrt{N}) & = 0.95,
\end{align}
and do some algebra
\begin{align}
P(a \sqrt{\frac{N}{\sigma^2}} \le  \mathcal{N}(0,1) \le  b \sqrt{\frac{N}{\sigma^2}}) & = 0.95,
\end{align}

Finally we find $a = -1.96 \sqrt{\frac{\sigma^2}{N}}$ and $b = 1.96 \sqrt{\frac{\sigma^2}{N}}$.  Now, let's see what $N$ has to be to keep the CI to within $5$.  We need $a=-5$, $b=5$.
```{r}
N <- (1.96*sqrt(sigma2)/5)^2
print(N)
```

We only need roughly $N=83$ samples to get within $\pm 5$ of $E[X]$.  Above we used $N=10000$, much more than needed.  Let's see what happens when we use $N=83$.
```{r, cache=T}
N <- 83
samples <- replicate(N, simulate_num_rolls(P_CL))
# average number of rolls
print(mean(samples))
```

Indeed, not very different than when we used $N=10000$.
